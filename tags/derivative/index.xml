<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>derivative on Runner</title>
    <link>/tags/derivative/</link>
    <description>Recent content in derivative on Runner</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <lastBuildDate>Mon, 04 Nov 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/derivative/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Jacobi&#39;s formula</title>
      <link>/posts/2019-11-04-matrix-cookbook-46/</link>
      <pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/2019-11-04-matrix-cookbook-46/</guid>
      <description>\[\tag{46}\frac{\partial \det \left( \mathbf{Y} \right)}{\partial x}=\,\,\det \left( \mathbf{Y} \right) Tr\left[ \mathbf{Y}^{-1}\frac{\partial \mathbf{Y}}{\partial x} \right] \]Formula \((46)\) is actually Jacobi’s formula. 1
Analogy in functions
For a differentiable function \(f: D\subseteq R\rightarrow R\), for all \(x\) in some neighborhood of \(a\), \(f\) can be written as: 2\[f(x)=f(a)+f^{\prime}(a) (x−a)+R(x−a) \]and, \(L(x)=f(a)+f^{\prime}(a)(x−a)\) is the best affine approximation of the function \(f\) at \(a\).
or, the idea could be expressed in other way:\[f(x+\epsilon)=f(x)+f^{\prime}(x) \epsilon +R\epsilon \]</description>
    </item>
    
    <item>
      <title>Derivative of log of determinant</title>
      <link>/posts/2019-10-30-matrix-cookbook-43/</link>
      <pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/2019-10-30-matrix-cookbook-43/</guid>
      <description>\[\begin{equation}\tag{43}\partial(\ln (\operatorname{det}(\mathbf{X})))=\operatorname{Tr}\left(\mathbf{X}^{-1} \partial \mathbf{X}\right)\end{equation}\]
Lemma 1
\[\begin{equation}\sum_{i} \sum_{j} \mathbf{A}^{\mathrm{T}}_{i j} \mathbf{B}_{i j} = \operatorname{Tr}\left(\mathbf{A} \mathbf{B}\right)\end{equation}\]
Lemma 2 1
(Credit to https://statisticaloddsandends.wordpress.com/2018/05/24/derivative-of-log-det-x/)
\[\begin{equation}\frac{\partial(\operatorname{det} \mathbf{X})}{\partial \mathbf{X}_{i j}}=\mathbf{C}_{i j}\end{equation}\]
For a matrix \(X\), we define some terms:
The \((i,j)\) minor of \(X\), denoted \(M_{ij}\), is the determinant of the \((n-1) \times (n-1)\) matrix that remains after removing the \(i\)th row and \(j\)th column from \(X\).</description>
    </item>
    
    <item>
      <title>Derivative of inverse matrix</title>
      <link>/posts/2019-10-24-matrix-derivative/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/2019-10-24-matrix-derivative/</guid>
      <description>\[\begin{equation}\tag{40}\partial\left(\mathbf{X}^{-1}\right)=-\mathbf{X}^{-1}(\partial \mathbf{X}) \mathbf{X}^{-1}\end{equation}\]
Explanation: 1
\[\begin{equation}\underbrace{(I)^{\prime}}_{=0}=\left(\mathbf{X} \mathbf{X}^{-1}\right)^{\prime}=\mathbf{X}^{\prime} \mathbf{X}^{-1}+\mathbf{X}\left(\mathbf{X}^{-1}\right)^{\prime} \Rightarrow\end{equation}\]
\[\begin{equation}\mathbf{X}\left(\mathbf{X}^{-1}\right)^{\prime}=-\mathbf{X}^{\prime} \mathbf{X}^{-1} \quad \Rightarrow\end{equation}\]
\[\begin{equation}\left(\mathbf{X}^{-1}\right)^{\prime}=-\mathbf{X}^{-1} \mathbf{X}^{\prime} \mathbf{X}^{-1}\end{equation}\]
\[\begin{equation}\tag{41}\partial(\operatorname{det}(\mathbf{X}))=\operatorname{Tr}(\operatorname{adj}(\mathbf{X}) \partial \mathbf{X})\end{equation}\]
BackgroundAdjugate MatrixThe adjugate of \(A\) is the transpose of the cofactor matrix \(C\) of \(X\),\[\begin{equation}\operatorname{adj}(\mathbf{X})=\mathbf{C}^{\top}\end{equation}\]
and,\[\begin{equation}\mathbf{X}^{-1}=\operatorname{det}(\mathbf{X})^{-1} \operatorname{adj}(\mathbf{X}) \quad \Rightarrow\end{equation}\]
\[\begin{equation}\operatorname{det}(\mathbf{X}) \mathbf{I} = \operatorname{adj}(\mathbf{X}) \mathbf{X}\end{equation}\]
Characteristic PolynomialThe characteristic polynomial of a square matrix is a polynomial which is invariant under matrix similarity and has the eigenvalues as roots.</description>
    </item>
    
  </channel>
</rss>