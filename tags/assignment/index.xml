<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Assignment on Runner</title>
    <link>/tags/assignment/</link>
    <description>Recent content in Assignment on Runner</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <lastBuildDate>Mon, 07 Nov 2016 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/assignment/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Exercise 4 - Neural Network Learning</title>
      <link>/posts/2016-11-07-exercise4-neural-network-learning/</link>
      <pubDate>Mon, 07 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/posts/2016-11-07-exercise4-neural-network-learning/</guid>
      <description>1. Neural NetworksFeedforward and const functionThe cost function for the neural network (without regularization) is
\[J(\theta) = \frac{1}{m}\sum_{i=1}^m\sum_{k=1}^K\bigg[−y_k^{(i)}\log((h_{θ}(x^{(i)}))_k)−(1−y_k^{(i)})\log(1−(h_θ(x^{(i)}))_k)\bigg]\],
where \(h_{\theta}(x^{(i)})\) is computed as shown in the Figure 2 and \(K = 10\) is the total number of possible labels. Note that \(h_θ(x^{(i)})_k = a^{(3)}_k\) is the activation (output value) of the \(k\)-th output unit.
Implementation-nnCostFunction.m
a1 = [ones(m, 1) X];z2 = a1*Theta1&amp;#39;;a2 = [ones(size(z2, 1), 1) sigmoid(z2)];z3 = a2*Theta2&amp;#39;;a3 = sigmoid(z3);yd = eye(num_labels);y = yd(y,:);log_dif = -log(a3).</description>
    </item>
    
    <item>
      <title>Exercise 3 - Multi-class Classification</title>
      <link>/posts/2016-10-23-exercise3-multi-class-classification/</link>
      <pubDate>Sun, 23 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/posts/2016-10-23-exercise3-multi-class-classification/</guid>
      <description>Cost function, gradient of regularized logistic regression for multi-class classification are similar to exercise 2.
This exercise implement one-vs-all classification by training multiple regularized logistic regression classifiers, one for each of the K classes in our dataset.
oneVsAll.m
function [all_theta] = oneVsAll(X, y, num_labels, lambda)%ONEVSALL trains multiple logistic regression classifiers and returns all%the classifiers in a matrix all_theta, where the i-th row of all_theta %corresponds to the classifier for label i% [all_theta] = ONEVSALL(X, y, num_labels, lambda) trains num_labels% logisitc regression classifiers and returns each of these classifiers% in a matrix all_theta, where the i-th row of all_theta corresponds % to the classifier for label i% Some useful variablesm = size(X, 1);n = size(X, 2);% You need to return the following variables correctly all_theta = zeros(num_labels, n + 1);% Add ones to the X data matrixX = [ones(m, 1) X];% ====================== YOUR CODE HERE ======================% Instructions: You should complete the following code to train num_labels% logistic regression classifiers with regularization% parameter lambda.</description>
    </item>
    
    <item>
      <title>Exercise 3 - Neural Networks</title>
      <link>/posts/2016-10-23-exercise3-neural-networks/</link>
      <pubDate>Sun, 23 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/posts/2016-10-23-exercise3-neural-networks/</guid>
      <description>Logistic regression cannot form more complex hypotheses as it is only a linear classifier1.
You could add more features (such as polynomial features) to logistic regression, but that can be very expensive to train.↩
</description>
    </item>
    
    <item>
      <title>Exercise 2 - Logistic Regression (1)</title>
      <link>/posts/2016-10-19-exercise2-logistic-regression/</link>
      <pubDate>Wed, 19 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/posts/2016-10-19-exercise2-logistic-regression/</guid>
      <description>1 Logistic Regression1.1 Visualizing dataplotdata.mfunction plotData(X, y)%PLOTDATA Plots the data points X and y into a new figure % PLOTDATA(x,y) plots the data points with + for the positive examples% and o for the negative examples. X is assumed to be a Mx2 matrix.% Create New Figurefigure; hold on;% ====================== YOUR CODE HERE ======================% Instructions: Plot the positive and negative examples on a% 2D plot, using the option &amp;#39;k+&amp;#39; for the positive% examples and &amp;#39;ko&amp;#39; for the negative examples.</description>
    </item>
    
    <item>
      <title>Exercise 2 - Logistic Regression (2)</title>
      <link>/posts/2016-10-21-exercise2-regularized-logistic-regression/</link>
      <pubDate>Wed, 19 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/posts/2016-10-21-exercise2-regularized-logistic-regression/</guid>
      <description>2 Regularized Logistic Regression2.1 Visualizing the data{: .image-center} Figure shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straightforward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.
2.2 Feature mappingOne way to fit the data better is to create more features from each data point.</description>
    </item>
    
    <item>
      <title>Exercise 1- Linear Regression</title>
      <link>/posts/2016-10-17-exercise1-linear-regression/</link>
      <pubDate>Mon, 17 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/posts/2016-10-17-exercise1-linear-regression/</guid>
      <description>1. Sublime Text3 Octave build system{&amp;quot;cmd&amp;quot;: [&amp;quot;octave-gui&amp;quot;, &amp;quot;$file&amp;quot;],&amp;quot;shell&amp;quot;: true // to show plots}2. Linear regression with one variableplotdata.mfunction plotData(x, y)%PLOTDATA Plots the data points x and y into a new figure % PLOTDATA(x,y) plots the data points and gives the figure axes labels of% population and profit.% ====================== YOUR CODE HERE ======================% Instructions: Plot the training data into a figure using the % &amp;quot;figure&amp;quot; and &amp;quot;plot&amp;quot; commands.</description>
    </item>
    
  </channel>
</rss>