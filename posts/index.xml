<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Runner</title>
    <link>/posts/</link>
    <description>Recent content in Posts on Runner</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <lastBuildDate>Fri, 11 Oct 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Matrix cookbook - determinant</title>
      <link>/posts/2019-10-17-matrix-cookbook-1.2-determinant/</link>
      <pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/2019-10-17-matrix-cookbook-1.2-determinant/</guid>
      <description>\[\begin{equation}\tag{18}\operatorname{det}(\mathbf{A})=\prod_{i} \lambda_{i} \quad \lambda_{i}=\operatorname{eig}(\mathbf{A})\end{equation}\]
\[\begin{equation}\tag{19}\operatorname{det}(c \mathbf{A})=c^{n} \operatorname{det}(\mathbf{A}), \quad \text { if } \mathbf{A} \in \mathbb{R}^{n \times n}\end{equation}\]
\[\begin{equation}\tag{20}\operatorname{det}\left(\mathbf{A}^{T}\right)=\operatorname{det}(\mathbf{A})\end{equation}\]
\[\begin{equation}\tag{21}\operatorname{det}(\mathbf{A B})=\operatorname{det}(\mathbf{A}) \operatorname{det}(\mathbf{B})\end{equation}\]
The determinant of a tranformation matrix is the scale of area/volume of the shape before and after the tranformation. \(\mathbf{A B}\) are two consecutive transformations, therefore its determinant is the product of two scales.
\[\begin{equation}\tag{22}\operatorname{det}\left(\mathbf{A}^{-1}\right)=1 / \operatorname{det}(\mathbf{A})\end{equation}\]</description>
    </item>
    
    <item>
      <title>Matrix cookbook - Trace</title>
      <link>/posts/2019-10-11-matrix-cookbook-trace/</link>
      <pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/2019-10-11-matrix-cookbook-trace/</guid>
      <description>\[\begin{equation}\tag{11}\operatorname{Tr}(\mathbf{A})=\sum_{i} A_{i i}\end{equation}\]
Let’s write the trace in a more convenient way. We have: 1\[\begin{equation}A e_{i}=\left[\begin{array}{ccc}{a_{11}} &amp;amp; {\cdots} &amp;amp; {a_{1 n}} \\ {\vdots} &amp;amp; {\ddots} &amp;amp; {\vdots} \\ {a_{n 1}} &amp;amp; {\cdots} &amp;amp; {a_{n n}}\end{array}\right]\left[\begin{array}{c}{0} \\ {\vdots} \\ {1} \\ {\vdots} \\ {0}\end{array}\right]=\left[\begin{array}{c}{a_{i 1}} \\ {\vdots} \\ {a_{i n}}\end{array}\right]\end{equation}\]where the \(1\) is in the \(i\)-th entry. This way:\[\begin{equation}\left\langle e_{i}, A e_{i}\right\rangle= e_{i}^{t} A e_{i}=A_{i i}\end{equation}\]So \(\operatorname{Tr}(\mathbf{A})=\sum_{i}A_{ii}\).</description>
    </item>
    
    <item>
      <title>Matrix cookbook - Basics</title>
      <link>/posts/2019-10-10-matrix-cookbook/</link>
      <pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/2019-10-10-matrix-cookbook/</guid>
      <description>\[\begin{equation}\tag{1}(\mathbf{A B})^{-1}=\mathbf{B}^{-1} \mathbf{A}^{-1}\end{equation}\]
\[\begin{equation}\tag{2}(\mathbf{A B C} \ldots)^{-1}=\ldots \mathbf{C}^{-1} \mathbf{B}^{-1} \mathbf{A}^{-1}\end{equation}\]
\[\begin{equation}\tag{3}\left(\mathbf{A}^{T}\right)^{-1}=\left(\mathbf{A}^{-1}\right)^{T}\end{equation}\]
\[\begin{equation}\tag{4}(\mathbf{A}+\mathbf{B})^{T}=\mathbf{A}^{T}+\mathbf{B}^{T}\end{equation}\]
\[\begin{equation}\tag{5}(\mathbf{A B})^{T}=\mathbf{B}^{T} \mathbf{A}^{T}\end{equation}\]
\[\begin{equation}\tag{6}(\mathbf{A B C} \ldots)^{T}=\ldots \mathbf{C}^{T} \mathbf{B}^{T} \mathbf{A}^{T}\end{equation}\]</description>
    </item>
    
    <item>
      <title>The Element of Statistic Learning - Chapter 5</title>
      <link>/posts/2019-10-07-esl-note/</link>
      <pubDate>Mon, 07 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/2019-10-07-esl-note/</guid>
      <description>NotesEigen-decomposition\[S = \sum_{k=1}^N \rho_ku_ku_k^T\]</description>
    </item>
    
    <item>
      <title>Exercise 4 - Neural Network Learning</title>
      <link>/posts/2016-11-07-exercise4-neural-network-learning/</link>
      <pubDate>Mon, 07 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/posts/2016-11-07-exercise4-neural-network-learning/</guid>
      <description>1. Neural Networks Feedforward and const function The cost function for the neural network (without regularization) is
\[ J(\theta) = \frac{1}{m}\sum_{i=1}^m\sum_{k=1}^K \bigg[ −y_k^{(i)}\log((h_{θ}(x^{(i)}))_k)−(1−y_k^{(i)})\log(1−(h_θ(x^{(i)}))_k) \bigg] \],
where \(h_{\theta}(x^{(i)})\) is computed as shown in the Figure 2 and \(K = 10\) is the total number of possible labels. Note that \(h_θ(x^{(i)})_k = a^{(3)}_k\) is the activation (output value) of the \(k\)-th output unit.
Implementation-nnCostFunction.m
a1 = [ones(m, 1) X]; z2 = a1*Theta1&amp;#39;; a2 = [ones(size(z2, 1), 1) sigmoid(z2)]; z3 = a2*Theta2&amp;#39;; a3 = sigmoid(z3); yd = eye(num_labels); y = yd(y,:); log_dif = -log(a3).</description>
    </item>
    
    <item>
      <title>Octave in Sublime Text3</title>
      <link>/posts/2016-11-06-octave/</link>
      <pubDate>Sun, 06 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/posts/2016-11-06-octave/</guid>
      <description> Install Octave Download Octave from https://ftp.gnu.org/gnu/octave/windows/, and register its folder to environment variable Path.
 Create build system for Octave file in Sublime Text3 Octave.sublime_build
{ &amp;quot;cmd&amp;quot;: [&amp;quot;octave-gui&amp;quot;, &amp;quot;$file&amp;quot;], &amp;quot;shell&amp;quot;: true // to show plots }  Create short-cut for canceling build Add the line to Preferencesbindings
{ &amp;quot;keys&amp;quot;: [&amp;quot;ctrl+shift+b&amp;quot;], &amp;quot;command&amp;quot;: &amp;quot;exec&amp;quot;, &amp;quot;args&amp;quot;: {&amp;quot;kill&amp;quot;: true} },  </description>
    </item>
    
    <item>
      <title>Exercise 3 - Multi-class Classification</title>
      <link>/posts/2016-10-23-exercise3-multi-class-classification/</link>
      <pubDate>Sun, 23 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/posts/2016-10-23-exercise3-multi-class-classification/</guid>
      <description>Cost function, gradient of regularized logistic regression for multi-class classification are similar to exercise 2.
This exercise implement one-vs-all classification by training multiple regularized logistic regression classifiers, one for each of the K classes in our dataset.
oneVsAll.m
function [all_theta] = oneVsAll(X, y, num_labels, lambda) %ONEVSALL trains multiple logistic regression classifiers and returns all %the classifiers in a matrix all_theta, where the i-th row of all_theta %corresponds to the classifier for label i % [all_theta] = ONEVSALL(X, y, num_labels, lambda) trains num_labels % logisitc regression classifiers and returns each of these classifiers % in a matrix all_theta, where the i-th row of all_theta corresponds % to the classifier for label i % Some useful variables m = size(X, 1); n = size(X, 2); % You need to return the following variables correctly all_theta = zeros(num_labels, n + 1); % Add ones to the X data matrix X = [ones(m, 1) X]; % ====================== YOUR CODE HERE ====================== % Instructions: You should complete the following code to train num_labels % logistic regression classifiers with regularization % parameter lambda.</description>
    </item>
    
    <item>
      <title>Exercise 3 - Neural Networks</title>
      <link>/posts/2016-10-23-exercise3-neural-networks/</link>
      <pubDate>Sun, 23 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/posts/2016-10-23-exercise3-neural-networks/</guid>
      <description> Logistic regression cannot form more complex hypotheses as it is only a linear classifier1.
 You could add more features (such as polynomial features) to logistic regression, but that can be very expensive to train.↩
   </description>
    </item>
    
    <item>
      <title>Exercise 2 - Logistic Regression (1)</title>
      <link>/posts/2016-10-19-exercise2-logistic-regression/</link>
      <pubDate>Wed, 19 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/posts/2016-10-19-exercise2-logistic-regression/</guid>
      <description>1 Logistic Regression 1.1 Visualizing data plotdata.m function plotData(X, y) %PLOTDATA Plots the data points X and y into a new figure % PLOTDATA(x,y) plots the data points with + for the positive examples % and o for the negative examples. X is assumed to be a Mx2 matrix. % Create New Figure figure; hold on; % ====================== YOUR CODE HERE ====================== % Instructions: Plot the positive and negative examples on a % 2D plot, using the option &amp;#39;k+&amp;#39; for the positive % examples and &amp;#39;ko&amp;#39; for the negative examples.</description>
    </item>
    
    <item>
      <title>Exercise 2 - Logistic Regression (2)</title>
      <link>/posts/2016-10-21-exercise2-regularized-logistic-regression/</link>
      <pubDate>Wed, 19 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/posts/2016-10-21-exercise2-regularized-logistic-regression/</guid>
      <description>2 Regularized Logistic Regression 2.1 Visualizing the data {: .image-center} Figure shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straightforward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.
 2.2 Feature mapping One way to fit the data better is to create more features from each data point.</description>
    </item>
    
    <item>
      <title>Exercise 1- Linear Regression</title>
      <link>/posts/2016-10-17-exercise1-linear-regression/</link>
      <pubDate>Mon, 17 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/posts/2016-10-17-exercise1-linear-regression/</guid>
      <description>1. Sublime Text3 Octave build system { &amp;quot;cmd&amp;quot;: [&amp;quot;octave-gui&amp;quot;, &amp;quot;$file&amp;quot;], &amp;quot;shell&amp;quot;: true // to show plots }  2. Linear regression with one variable plotdata.m function plotData(x, y) %PLOTDATA Plots the data points x and y into a new figure % PLOTDATA(x,y) plots the data points and gives the figure axes labels of % population and profit. % ====================== YOUR CODE HERE ====================== % Instructions: Plot the training data into a figure using the % &amp;quot;figure&amp;quot; and &amp;quot;plot&amp;quot; commands.</description>
    </item>
    
    <item>
      <title>R markdown in Sublime Text3</title>
      <link>/posts/2016-08-10-r-markdown/</link>
      <pubDate>Thu, 11 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/posts/2016-08-10-r-markdown/</guid>
      <description>1. Install Sublime Text plugin randy3k/R-box  2 Windows 2.1. Create my own sublime-build for R markdown files The default build system of R-box doesn’t work, and get the error
Error: &amp;#39;\G&amp;#39; is an unrecognized escape in character string starting &amp;quot;&amp;#39;C:\G&amp;quot; Execution halted [Finished in 0.4s with exit code 1] since the windows path escape is not correctly handled.
This issue can be resolved by regular expression replacement 1</description>
    </item>
    
    <item>
      <title>First Post</title>
      <link>/posts/2016-07-30-first-post/</link>
      <pubDate>Sat, 30 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/posts/2016-07-30-first-post/</guid>
      <description>Below is just about everything you’ll need to style in the theme. Check the source code to see the many embedded elements within paragraphs.
Mathjax 1 LaTeX math delimiters: \\(a^2 + b^2 = c^2\\) or “\(a^2 + b^2 = c^2\)” for inline math \(a^2 + b^2 = c^2\); and \\[a^2 + b^2 = c^2\\] for displayed equations \[a^2 + b^2 = c^2\].
 Heading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6     Body text Lorem ipsum dolor sit amet, test link adipiscing elit.</description>
    </item>
    
  </channel>
</rss>