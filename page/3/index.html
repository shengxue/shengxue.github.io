<!DOCTYPE html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Runner</title>
<meta name="description" content="">

    





<meta name="twitter:title" content="Runner">
<meta name="twitter:description" content="Time and pressure ...">


    
    
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="/images/geiranger-1508630_1920.jpg">
    



<meta property="og:type" content="article">
<meta property="og:title" content="Runner">
<meta property="og:description" content="Time and pressure ...">
<meta property="og:url" content="/">
<meta property="og:site_name" content="Runner">


    
    <meta property="og:image" content="/images/geiranger-1508630_1920.jpg" />


  <meta property="og:updated_time" content="2019-11-04T00:00:00&#43;00:00"/>



<link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/monokai-sublime.min.css" rel="stylesheet">




<link rel="canonical" href="/">
<link href="/index.xml" rel="alternate" type="application/rss+xml" title="Runner" />
  <link href="/index.xml" rel="feed" type="application/rss+xml" title="Runner" />

<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">


<link rel="stylesheet" href="/css/main.css">
<link rel="stylesheet" href="/css/note.css">

<meta http-equiv="cleartype" content="on">

<meta name="generator" content="Hugo 0.58.3" />

<script src="/js/vendor/modernizr-2.6.2.custom.min.js"></script>


<link rel="shortcut icon" href="/favicon.png">



<link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/monokai.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<script type="text/javascript">
    WebFontConfig = {"google":{"families":["Lora:r:latin,latin-ext","Lora:r,i,b,bi:latin,latin-ext"]}};
    (function() {
      var wf = document.createElement('script');
      wf.src = 'https://s0.wp.com/wp-content/plugins/custom-fonts/js/webfont.js';
      wf.type = 'text/javascript';
      wf.async = 'true';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(wf, s);
      })();
</script>

</head>

<body id="post-index" class="feature">
<nav id="dl-menu" class="dl-menuwrapper" role="navigation" style="display:inline-block">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
			
				<li>
					
					<img src="/images/shengxue.jpg" alt="Sheng Xue's photo" class="author-photo">
					
					<h4>Sheng Xue</h4>
					<p>Working as C&#43;&#43;/C# developer, while actively learning node.js/AngularJs/Data Science/Machine Learning</p>
				</li>
				<li><a href="/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				
				
				
				
				
				<li>
					<a href="https://github.com/shengxue/shengxue.github.io"><i class="fa fa-fw fa-github"></i> GitHub</a>
				</li>
				
				
				
				
			
			</ul>
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="/posts/">All Posts</a></li>
				<li><a href="/tags/">All Tags</a></li>
			</ul>
		</li>
		
	    <li><a href="https://www.linkedin.com/in/sheng-xue-24550b28/" target="_blank">Linkedin</a></li>
	  
	</ul>
</nav>

<div class="entry-header">
  

	<div class="image-credit">Image credit: <a href="http://www.dargadgetz.com/ios-7-abstract-wallpaper-pack-for-iphone-5-and-ipod-touch-retina/">dargadgetz</a></div>
    <div class="entry-image">
      <img src="/images/geiranger-1508630_1920.jpg" alt="">
    </div>
  
  <div class="header-title">
    <div class="header-title-wrap">
		<h1><a href="/" title="Go to the homepage">Runner</a></h1>
	  <h2>
            Time and pressure ...
          </h2>
    </div>
  </div>
</div>

<div id="main" role="main">

<article class="hentry">
  <header>
    
    <div class="entry-meta">
         
		<span class="entry-date date published updated"><time datetime="2016-10-23 00:00:00 &#43;0000 UTC"><a href="/posts/2016-10-23-exercise3-multi-class-classification/">Oct 23, 2016</a></time></span>
        
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        Reading time ~3 minutes
      </span>
    </div>
    
	<h1 class="entry-title"><a href="/posts/2016-10-23-exercise3-multi-class-classification/" rel="bookmark" title="Exercise 3 - Multi-class Classification" itemprop="url">Exercise 3 - Multi-class Classification</a></h1>
    
  </header>
  <div class="entry-content">
    


<p>Cost function, gradient of regularized logistic regression for multi-class classification are similar to exercise 2.</p>
<p>This exercise implement one-vs-all classification by training multiple regularized logistic regression classifiers, one for each of the K classes in our dataset.</p>
<p><strong>oneVsAll.m</strong></p>
<pre class="m"><code>function [all_theta] = oneVsAll(X, y, num_labels, lambda)
%ONEVSALL trains multiple logistic regression classifiers and returns all
%the classifiers in a matrix all_theta, where the i-th row of all_theta 
%corresponds to the classifier for label i
%   [all_theta] = ONEVSALL(X, y, num_labels, lambda) trains num_labels
%   logisitc regression classifiers and returns each of these classifiers
%   in a matrix all_theta, where the i-th row of all_theta corresponds 
%   to the classifier for label i

% Some useful variables
m = size(X, 1);
n = size(X, 2);

% You need to return the following variables correctly 
all_theta = zeros(num_labels, n + 1);

% Add ones to the X data matrix
X = [ones(m, 1) X];

% ====================== YOUR CODE HERE ======================
% Instructions: You should complete the following code to train num_labels
%               logistic regression classifiers with regularization
%               parameter lambda. 
%
% Hint: theta(:) will return a column vector.
%
% Hint: You can use y == c to obtain a vector of 1&#39;s and 0&#39;s that tell use 
%       whether the ground truth is true/false for this class.
%
% Note: For this assignment, we recommend using fmincg to optimize the cost
%       function. It is okay to use a for-loop (for c = 1:num_labels) to
%       loop over the different classes.
%
%       fmincg works similarly to fminunc, but is more efficient when we
%       are dealing with large number of parameters.
%
% Example Code for fmincg:
%
%     % Set Initial theta
%     initial_theta = zeros(n + 1, 1);
%     
%     % Set options for fminunc
%     options = optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, 50);
% 
%     % Run fmincg to obtain the optimal theta
%     % This function will return theta and the cost 
%     [theta] = ...
%         fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ...
%                 initial_theta, options);
%

for i = 1:num_labels
    initial_theta = zeros(n+1, 1);
    options = optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, 50);
    [theta] = fmincg(@(t)(lrCostFunction(t, X, (y == i), lambda)), ...
        initial_theta, options);
    all_theta(i,:) = theta&#39;;
end

% =========================================================================

end
</code></pre>
<p><strong>predictOneVsAll.m</strong></p>
<pre class="m"><code>function p = predictOneVsAll(all_theta, X)
%PREDICT Predict the label for a trained one-vs-all classifier. The labels 
%are in the range 1..K, where K = size(all_theta, 1). 
%  p = PREDICTONEVSALL(all_theta, X) will return a vector of predictions
%  for each example in the matrix X. Note that X contains the examples in
%  rows. all_theta is a matrix where the i-th row is a trained logistic
%  regression theta vector for the i-th class. You should set p to a vector
%  of values from 1..K (e.g., p = [1; 3; 1; 2] predicts classes 1, 3, 1, 2
%  for 4 examples) 

m = size(X, 1);
num_labels = size(all_theta, 1);

% You need to return the following variables correctly 
p = zeros(size(X, 1), 1);

% Add ones to the X data matrix
X = [ones(m, 1) X];

% ====================== YOUR CODE HERE ======================
% Instructions: Complete the following code to make predictions using
%               your learned logistic regression parameters (one-vs-all).
%               You should set p to a vector of predictions (from 1 to
%               num_labels).
%
% Hint: This code can be done all vectorized using the max function.
%       In particular, the max function can also return the index of the 
%       max element, for more information see &#39;help max&#39;. If your examples 
%       are in rows, then, you can use max(A, [], 2) to obtain the max 
%       for each row.
%       

all_ps = sigmoid(X*all_theta&#39;);
[p_max,i_max] = max(all_ps, [], 2);
p = i_max;

% =========================================================================

end</code></pre>

  </div>
</article>

<article class="hentry">
  <header>
    
    <div class="entry-meta">
         
		<span class="entry-date date published updated"><time datetime="2016-10-23 00:00:00 &#43;0000 UTC"><a href="/posts/2016-10-23-exercise3-neural-networks/">Oct 23, 2016</a></time></span>
        
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        Reading time ~1 minute
      </span>
    </div>
    
	<h1 class="entry-title"><a href="/posts/2016-10-23-exercise3-neural-networks/" rel="bookmark" title="Exercise 3 - Neural Networks" itemprop="url">Exercise 3 - Neural Networks</a></h1>
    
  </header>
  <div class="entry-content">
    


<p>Logistic regression cannot form more complex hypotheses as it is only a linear classifier<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>You could add more features (such as polynomial features) to logistic regression, but that can be very expensive to train.<a href="#fnref1">↩</a></p></li>
</ol>
</div>

  </div>
</article>

<article class="hentry">
  <header>
    
    <div class="entry-meta">
         
		<span class="entry-date date published updated"><time datetime="2016-10-19 00:00:00 &#43;0000 UTC"><a href="/posts/2016-10-19-exercise2-logistic-regression/">Oct 19, 2016</a></time></span>
        
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        Reading time ~6 minutes
      </span>
    </div>
    
	<h1 class="entry-title"><a href="/posts/2016-10-19-exercise2-logistic-regression/" rel="bookmark" title="Exercise 2 - Logistic Regression (1)" itemprop="url">Exercise 2 - Logistic Regression (1)</a></h1>
    
  </header>
  <div class="entry-content">
    


<div id="logistic-regression" class="section level1">
<h1>1 Logistic Regression</h1>
<div id="visualizing-data" class="section level2">
<h2>1.1 Visualizing data</h2>
<div id="plotdata.m" class="section level3">
<h3>plotdata.m</h3>
<pre class="m"><code>function plotData(X, y)
%PLOTDATA Plots the data points X and y into a new figure 
%   PLOTDATA(x,y) plots the data points with + for the positive examples
%   and o for the negative examples. X is assumed to be a Mx2 matrix.

% Create New Figure
figure; hold on;

% ====================== YOUR CODE HERE ======================
% Instructions: Plot the positive and negative examples on a
%               2D plot, using the option &#39;k+&#39; for the positive
%               examples and &#39;ko&#39; for the negative examples.
%


% Find Indices of Positive and Negative Examples
pos = find(y==1); neg = find(y == 0);
% Plot Examples
plot(X(pos, 1), X(pos, 2), &#39;k+&#39;,&#39;LineWidth&#39;, 2, &#39;MarkerSize&#39;, 7);
plot(X(neg, 1), X(neg, 2), &#39;ko&#39;, &#39;MarkerFaceColor&#39;, &#39;y&#39;, ...
    &#39;MarkerSize&#39;, 7);

% =========================================================================

hold off;

end</code></pre>
</div>
</div>
<div id="sigmoid-function" class="section level2">
<h2>1.2 Sigmoid function</h2>
<p>Logistic regression hypothesis is defined as: <span class="math display">\[h_\theta(x) = g(\theta^Tx)\]</span>, where function <span class="math inline">\(g\)</span> is the sigmoid function. The sigmoid function is defined as: <span class="math display">\[g(z) = \frac{1}{1 + e^{-z}}\]</span></p>
<div id="sigmoid.m" class="section level3">
<h3>Sigmoid.m</h3>
<pre class="m"><code>function g = sigmoid(z)
%SIGMOID Compute sigmoid functoon
%   J = SIGMOID(z) computes the sigmoid of z.

% You need to return the following variables correctly 
g = zeros(size(z));

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the sigmoid of each value of z (z can be a matrix,
%               vector or scalar).

g = 1./(1 .+ exp(-1*z) );

% =============================================================

end</code></pre>
</div>
</div>
<div id="cost-function-and-gradient" class="section level2">
<h2>1.3 Cost function and gradient</h2>
<p>The cost function in logistic regression is <span class="math display">\[J(\theta) = \frac{1}{m}\sum_{i=1}^m[−y^{(i)}\log(h_\theta(x^{(i)})) − (1 − y^{(i)})\log(1 − h_\theta(x^{(i)}))]\]</span>, and the gradient of the cost is a vector of the same length as <span class="math inline">\(\theta\)</span> where the <span class="math inline">\(j\)</span>th element (for j = 0, 1, . . . , n) is defined as follows: <span class="math display">\[\frac{\partial J(\theta) }{\partial \theta_j } = \frac{1}{m}\sum_{i=1}^m \bigg(h_\theta(x^{(i)}) − y^{(i)}\bigg)x_j^{(i)}\]</span></p>
<div id="constfunction.m" class="section level3">
<h3>constFunction.m</h3>
<pre class="m"><code>function [J, grad] = costFunction(theta, X, y)
%COSTFUNCTION Compute cost and gradient for logistic regression
%   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the
%   parameter for logistic regression and the gradient of the cost
%   w.r.t. to the parameters.

% Initialize some useful values
m = length(y); % number of training examples

% You need to return the following variables correctly 
J = 0;
grad = zeros(size(theta));

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost of a particular choice of theta.
%               You should set J to the cost.
%               Compute the partial derivatives and set grad to the partial
%               derivatives of the cost w.r.t. each parameter in theta
%
% Note: grad should have the same dimensions as theta
%

h = sigmoid(X*theta);
J=(-y&#39;*log(h) - (1-y)&#39;*log(1.-h))/m;
grad = X&#39;*(h-y)/m;


% =============================================================

end</code></pre>
</div>
</div>
<div id="learning-parameters-using-fminunc" class="section level2">
<h2>1.4 Learning parameters using fminunc</h2>
<p>Octave/MATLAB’s fminunc is an optimization solver that finds the minimum of an unconstrained function. For logistic regression, you want to optimize the cost function <span class="math inline">\(J(\theta)\)</span> with parameters <span class="math inline">\(\theta\)</span>.</p>
<pre class="m"><code>%% ============= Part 3: Optimizing using fminunc  =============
%  In this exercise, you will use a built-in function (fminunc) to find the
%  optimal parameters theta.

%  Set options for fminunc
options = optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, 400);

%  Run fminunc to obtain the optimal theta
%  This function will return theta and the cost 
[theta, cost] = ...
    fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);</code></pre>
</div>
<div id="decision-boundary" class="section level2">
<h2>1.5 Decision boundary</h2>
<p>This final <span class="math inline">\(\theta\)</span> value computed from <strong>fminunc</strong> will then be used to plot the decision boundary on the training data.</p>
<p><span class="math inline">\(y\)</span> value on the decision boundary satifies: <span class="math display">\[y = h_\theta(x) = g\bigg(\theta^Tx\bigg) = 0.5 \]</span>, that is, <span class="math display">\[\theta^Tx = 0 \]</span></p>
<ul>
<li><p>When training data X has two features <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, <span class="math display">\[\theta_1 + \theta_2 * x_1 + \theta_3*x_2 = 0 \]</span>, that is, <span class="math display">\[x_2 = -\frac{\theta_1 + \theta_2 * x_1}{\theta_3} \]</span>,</p></li>
<li><p>When training data X has more than 2 features, how to visualize it on 2D plot?</p></li>
</ul>
<div id="plotdecisionboundary.m" class="section level3">
<h3>plotDecisionBoundary.m</h3>
<pre class="m"><code>function plotDecisionBoundary(theta, X, y)
%PLOTDECISIONBOUNDARY Plots the data points X and y into a new figure with
%the decision boundary defined by theta
%   PLOTDECISIONBOUNDARY(theta, X,y) plots the data points with + for the 
%   positive examples and o for the negative examples. X is assumed to be 
%   a either 
%   1) Mx3 matrix, where the first column is an all-ones column for the 
%      intercept.
%   2) MxN, N&gt;3 matrix, where the first column is all-ones

% Plot Data
plotData(X(:,2:3), y);
hold on

if size(X, 2) &lt;= 3
    % Only need 2 points to define a line, so choose two endpoints
    plot_x = [min(X(:,2))-2,  max(X(:,2))+2];

    % Calculate the decision boundary line
    plot_y = (-1./theta(3)).*(theta(2).*plot_x + theta(1));

    % Plot, and adjust axes for better viewing
    plot(plot_x, plot_y)
    
    % Legend, specific for the exercise
    legend(&#39;Admitted&#39;, &#39;Not admitted&#39;, &#39;Decision Boundary&#39;)
    axis([30, 100, 30, 100])
else
    % Here is the grid range
    u = linspace(-1, 1.5, 50);
    v = linspace(-1, 1.5, 50);

    z = zeros(length(u), length(v));
    % Evaluate z = theta*x over the grid
    for i = 1:length(u)
        for j = 1:length(v)
            z(i,j) = mapFeature(u(i), v(j))*theta;
        end
    end
    z = z&#39;; % important to transpose z before calling contour

    % Plot z = 0
    % Notice you need to specify the range [0, 0]
    contour(u, v, z, [0, 0], &#39;LineWidth&#39;, 2)
end
hold off

end
</code></pre>
</div>
<div id="mapfeature.m" class="section level3">
<h3>mapFeature.m</h3>
<pre class="m"><code>function out = mapFeature(X1, X2)
% MAPFEATURE Feature mapping function to polynomial features
%
%   MAPFEATURE(X1, X2) maps the two input features
%   to quadratic features used in the regularization exercise.
%
%   Returns a new feature array with more features, comprising of 
%   X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..
%
%   Inputs X1, X2 must be the same size
%

degree = 6;
out = ones(size(X1(:,1)));
for i = 1:degree
    for j = 0:i
        out(:, end+1) = (X1.^(i-j)).*(X2.^j);
    end
end

end</code></pre>
</div>
</div>
<div id="evaluating-logistic-regression" class="section level2">
<h2>1.6 Evaluating logistic regression</h2>
<div id="predict.m" class="section level3">
<h3>predict.m</h3>
<pre class="m"><code>function p = predict(theta, X)
%PREDICT Predict whether the label is 0 or 1 using learned logistic 
%regression parameters theta
%   p = PREDICT(theta, X) computes the predictions for X using a 
%   threshold at 0.5 (i.e., if sigmoid(theta&#39;*x) &gt;= 0.5, predict 1)

m = size(X, 1); % Number of training examples

% You need to return the following variables correctly
p = zeros(m, 1);

% ====================== YOUR CODE HERE ======================
% Instructions: Complete the following code to make predictions using
%               your learned logistic regression parameters. 
%               You should set p to a vector of 0&#39;s and 1&#39;s
%

p = sigmoid(X, theta)&gt;=0.5;

% =========================================================================

end</code></pre>
<pre class="m"><code>%  Predict probability for a student with score 45 on exam 1 
%  and score 85 on exam 2 

prob = sigmoid([1 45 85] * theta);
fprintf([&#39;For a student with scores 45 and 85, we predict an admission &#39; ...
         &#39;probability of %f\n\n&#39;], prob);

% Compute accuracy on our training set
p = predict(theta, X);

fprintf(&#39;Train Accuracy: %f\n&#39;, mean(double(p == y)) * 100);</code></pre>
</div>
</div>
</div>

  </div>
</article>

<article class="hentry">
  <header>
    
    <div class="entry-meta">
         
		<span class="entry-date date published updated"><time datetime="2016-10-19 00:00:00 &#43;0000 UTC"><a href="/posts/2016-10-21-exercise2-regularized-logistic-regression/">Oct 19, 2016</a></time></span>
        
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        Reading time ~2 minutes
      </span>
    </div>
    
	<h1 class="entry-title"><a href="/posts/2016-10-21-exercise2-regularized-logistic-regression/" rel="bookmark" title="Exercise 2 - Logistic Regression (2)" itemprop="url">Exercise 2 - Logistic Regression (2)</a></h1>
    
  </header>
  <div class="entry-content">
    


<div id="regularized-logistic-regression" class="section level1">
<h1>2 Regularized Logistic Regression</h1>
<div id="visualizing-the-data" class="section level2">
<h2>2.1 Visualizing the data</h2>
<p><img src="/images/exercise2-2.png" alt="Data Image" /> {: .image-center} Figure shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straightforward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.</p>
</div>
<div id="feature-mapping" class="section level2">
<h2>2.2 Feature mapping</h2>
<p>One way to fit the data better is to create more features from each data point. In the provided function <strong>mapFeature.m</strong>, we will map the features into all polynomial terms of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> up to the sixth power. <span class="math display">\[
feature(x) = \begin{bmatrix} \\\ 1 \\\ x_1 \\\ x_2 \\\ x_1^2  \\\ x_1x_2 \\\ x_2^2 \\\ x1^3 \\\ . \\\ . \\\ . \\\ x_1x_2^5 \\\ x_2^6
                \end{bmatrix}
\]</span></p>
</div>
<div id="cost-function-and-gradient" class="section level2">
<h2>2.3 Cost function and gradient</h2>
<p>The regularized cost function in logistic regression is <span class="math display">\[J(\theta) = \frac{1}{m}\sum_{i=1}^m[−y^{(i)}\log(h_\theta(x^{(i)})) − (1 − y^{(i)})\log(1 − h_\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2\]</span></p>
<p>The gradient of the cost function is a vector where the <span class="math inline">\(j\)</span>th element is defined as follows:</p>
<p><span class="math display">\[
\begin{array}{lr}
\frac{\partial J(\theta)}{\partial \theta_0} = \frac{1}{m}\sum_{i=1}^m\bigg(h_\theta(x^{(i)}) − y^{(i)}\bigg)x_j^{(i)} &amp; \text{for }j = 0 \\\
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m}\sum_{i=1}^m\bigg(h_\theta(x^{(i)}) − y^{(i)}\bigg)x_j^{(i)} + \frac{\lambda}{m}\theta_j &amp; \text{for }j \geq 1
\end{array}
\]</span></p>
<p><strong>costFunctionReg.m</strong></p>
<pre class="m"><code>function [J, grad] = costFunctionReg(theta, X, y, lambda)
%COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization
%   J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using
%   theta as the parameter for regularized logistic regression and the
%   gradient of the cost w.r.t. to the parameters. 

% Initialize some useful values
m = length(y); % number of training examples

% You need to return the following variables correctly 
J = 0;
grad = zeros(size(theta));

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost of a particular choice of theta.
%               You should set J to the cost.
%               Compute the partial derivatives and set grad to the partial
%               derivatives of the cost w.r.t. each parameter in theta


n = size(theta);
h = sigmoid(X*theta);

theta1 = [0 ; theta(2:n)];
p = lambda*(theta1&#39;*theta1)/(2*m);

J=(-y&#39;*log(h) - (1-y)&#39;*log(1-h))/m + p;

grad = X&#39;*(h-y)/m + lambda*theta1/m;
% =============================================================

end</code></pre>
<div id="learning-parameters-using-fiminunc" class="section level3">
<h3>2.3.1 Learning parameters using fiminunc</h3>
</div>
</div>
<div id="plotting-the-decision-boundary" class="section level2">
<h2>2.4 Plotting the decision boundary</h2>
<p><strong>plotDecisionBoundary.m</strong></p>
</div>
</div>

  </div>
</article>

<article class="hentry">
  <header>
    
    <div class="entry-meta">
         
		<span class="entry-date date published updated"><time datetime="2016-10-17 00:00:00 &#43;0000 UTC"><a href="/posts/2016-10-17-exercise1-linear-regression/">Oct 17, 2016</a></time></span>
        
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        Reading time ~4 minutes
      </span>
    </div>
    
	<h1 class="entry-title"><a href="/posts/2016-10-17-exercise1-linear-regression/" rel="bookmark" title="Exercise 1- Linear Regression" itemprop="url">Exercise 1- Linear Regression</a></h1>
    
  </header>
  <div class="entry-content">
    


<div id="sublime-text3-octave-build-system" class="section level1">
<h1>1. Sublime Text3 Octave build system</h1>
<pre class="json"><code>{
    &quot;cmd&quot;: [&quot;octave-gui&quot;, &quot;$file&quot;],
    &quot;shell&quot;: true    // to show plots
}</code></pre>
</div>
<div id="linear-regression-with-one-variable" class="section level1">
<h1>2. Linear regression with one variable</h1>
<div id="plotdata.m" class="section level2">
<h2>plotdata.m</h2>
<pre class="m"><code>function plotData(x, y)
%PLOTDATA Plots the data points x and y into a new figure 
%   PLOTDATA(x,y) plots the data points and gives the figure axes labels of
%   population and profit.

% ====================== YOUR CODE HERE ======================
% Instructions: Plot the training data into a figure using the 
%               &quot;figure&quot; and &quot;plot&quot; commands. Set the axes labels using
%               the &quot;xlabel&quot; and &quot;ylabel&quot; commands. Assume the 
%               population and revenue data have been passed in
%               as the x and y arguments of this function.
%
% Hint: You can use the &#39;rx&#39; option with plot to have the markers
%       appear as red crosses. Furthermore, you can make the
%       markers larger by using plot(..., &#39;rx&#39;, &#39;MarkerSize&#39;, 10);

figure; % open a new figure window


plot(x, y, &#39;rx&#39;, &#39;MarkerSize&#39;, 10);
ylabel(&#39;Profit in $10,000s&#39;);
xlabel(&#39;Population of City in 10,000s&#39;);



% ============================================================

end</code></pre>
</div>
<div id="cost-function" class="section level2">
<h2>Cost function</h2>
<div id="equation" class="section level3">
<h3>Equation</h3>
<p>The objective of linear regression is to minimize the cost function <span class="math inline">\(J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2\)</span> where the hypothesis <span class="math inline">\(h_\theta(x)\)</span> is given by the linear model <span class="math display">\[h_\theta(x) = \theta^T x = \theta_0 + \theta_1\]</span></p>
</div>
<div id="implementation" class="section level3">
<h3>Implementation</h3>
<pre class="m"><code>% non-vectorized version.
J = 0;
for i=1:m
  dif = X(i, :)*theta-y(i);
  J = J + dif*dif;
endfor
J = J / (2*m);


% vectorized version.
dif = X*theta-y;
J = (dif&#39;*dif)/(2*m);</code></pre>
</div>
</div>
<div id="gradient-decent" class="section level2">
<h2>Gradient decent</h2>
<div id="equation-1" class="section level3">
<h3>Equation</h3>
<p><span class="math display">\[\theta_j = \theta_j - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}\]</span></p>
</div>
<div id="implementation-1" class="section level3">
<h3>Implementation</h3>
<ul>
<li>Version (1)</li>
</ul>
<pre class="m"><code>function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)
%GRADIENTDESCENT Performs gradient descent to learn theta
%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by 
%   taking num_iters gradient steps with learning rate alpha

% Initialize some useful values
m = length(y); % number of training examples
J_history = zeros(num_iters, 1);

for iter = 1:num_iters

    % ====================== YOUR CODE HERE ======================
    % Instructions: Perform a single gradient step on the parameter vector
    %               theta. 
    %
    % Hint: While debugging, it can be useful to print out the values
    %       of the cost function (computeCost) and gradient here.
    %
    
    theta_prev = theta;
    p = length(theta);

    for j = 1:p

        sum = 0;
        for i = 1:m
            sum = sum + (X(i,:)*theta_prev - y(i))*X(i,j);
        end

        derive = sum/m;
        theta(j) = theta(j) - alpha*derive;
    end


    % ============================================================

    % Save the cost J in every iteration    
    J_history(iter) = computeCost(X, y, theta);

end

end</code></pre>
<ul>
<li>version (2)</li>
</ul>
<pre class="m"><code>    theta_prev = theta;
    p = length(theta);

    for j = 1:p

        derive = (X*theta_prev - y)&#39;*X(:,j)/m;
        theta(j) -= alpha*derive;
    end</code></pre>
<ul>
<li>Vectorized version</li>
</ul>
<pre class="m"><code>theta -= alpha*X&#39;*(X*theta-y)/m;</code></pre>
</div>
</div>
</div>
<div id="linear-regression-with-multiple-variable" class="section level1">
<h1>3. Linear regression with multiple variable</h1>
<div id="feature-normalization" class="section level2">
<h2>Feature normalization</h2>
<p>When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly.</p>
<ul>
<li>Non-vectorized version</li>
</ul>
<pre class="m"><code>function [X_norm, mu, sigma] = featureNormalize(X)
%FEATURENORMALIZE Normalizes the features in X 
%   FEATURENORMALIZE(X) returns a normalized version of X where
%   the mean value of each feature is 0 and the standard deviation
%   is 1. This is often a good preprocessing step to do when
%   working with learning algorithms.

% You need to set these values correctly
X_norm = X;
mu = zeros(1, size(X, 2));
sigma = zeros(1, size(X, 2));

% ====================== YOUR CODE HERE ======================
% Instructions: First, for each feature dimension, compute the mean
%               of the feature and subtract it from the dataset,
%               storing the mean value in mu. Next, compute the 
%               standard deviation of each feature and divide
%               each feature by it&#39;s standard deviation, storing
%               the standard deviation in sigma. 
%
%               Note that X is a matrix where each column is a 
%               feature and each row is an example. You need 
%               to perform the normalization separately for 
%               each feature. 
%
% Hint: You might find the &#39;mean&#39; and &#39;std&#39; functions useful.
%       

for p = 1:size(X, 2)
    mu(p) = mean(X(:, p), &quot;a&quot;);
    sigma(p) = std(X(:, p));
end

for p = 1:size(X, 2)
    for i = 1:size(X, 1)
      X_norm(i, p) = (X(i, p)-mu(p))/sigma(p);
    end
end

% ============================================================

end
</code></pre>
<ul>
<li>Vectorized version</li>
</ul>
<pre class="m"><code>    mu = mean(X, &quot;a&quot;);
    sigma = std(X);

    ones_matrix = ones(size(X));
    X_norm = (X - ones_matrix*diag(mu))./(ones_matrix*diag(sigma));</code></pre>
<p>Estimate the price of a 1650 sq-ft, 3 br house, in ex1_multi.m</p>
<pre class="m"><code>% ====================== YOUR CODE HERE ======================
% Recall that the first column of X is all-ones. Thus, it does
% not need to be normalized.
price = [1 ([1650 3] - mu)./sigma]*theta; % You should change this</code></pre>
</div>
<div id="normal-equations" class="section level2">
<h2>Normal Equations</h2>
<p>The closed-form solution to linear regression is <span class="math display">\[\theta = (X^TX)^{-1}X^T\vec{y}\]</span></p>
<ul>
<li>Implementation</li>
</ul>
<pre class="m"><code>function [theta] = normalEqn(X, y)
%NORMALEQN Computes the closed-form solution to linear regression 
%   NORMALEQN(X,y) computes the closed-form solution to linear 
%   regression using the normal equations.

theta = zeros(size(X, 2), 1);

% ====================== YOUR CODE HERE ======================
% Instructions: Complete the code to compute the closed form solution
%               to linear regression and put the result in theta.
%

% ---------------------- Sample Solution ----------------------


theta = inv(X&#39;*X)*X&#39;*y;

% -------------------------------------------------------------


% ============================================================

end</code></pre>
<p>Estimate the price of a 1650 sq-ft, 3 br house</p>
<pre class="m"><code>price = [1 1650 3]*theta; % You should change this</code></pre>
</div>
</div>

  </div>
</article>




<div class="pagination">
  <ul class="inline-list">
	  
    
        <li><a href="/page/2/" class="btn">Previous</a></li>
    

	

	
	
    
	<li><a href="/">1</a></li>
    
	
    
	<li><a href="/page/2/">2</a></li>
    
	
    
	<li><strong class="current-page">3</strong></li>
    
	
    
	<li><a href="/page/4/">4</a></li>
    
	

	

	
    
      <li><a href="/page/4/" class="btn">Next</a></li>
    
  </ul>
</div>


</div>

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span> Powered by <a href="https://gohugo.io/" rel="nofollow">Hugo</a> and blogdown using the <a href="https://github.com/dldx/hpstr-hugo-theme" rel="nofollow">HPSTR</a> theme.</span>

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-111479944-1', 'auto');
	
	ga('send', 'pageview');
}
</script>


<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "algorithmist" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/r.min.js"></script>

<script>
hljs.configure({languages: []});
hljs.initHighlightingOnLoad();
</script>

<script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  </footer>
</div>

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="\/js\/vendor\/jquery-1.9.1.min.js"><\/script>')</script>
<script src="/js/scripts.min.js"></script>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-111479944-1', 'auto');
	
	ga('send', 'pageview');
}
</script>



</body>
</html>

